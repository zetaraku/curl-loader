<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE faqs PUBLIC "-//APACHE//DTD FAQ V2.0//EN" "http://forrest.apache.org/dtd/faq-v20.dtd">

<faqs>
  <title>Frequently Asked Questions</title>
  <faqsection id="Terminology">
    <title>Basic Terminology</title>
    <faq id="batch">
      <question>
     What is the batch?
      </question>
      <answer>
      Batch is a group of clients (virtual clients emulating real clients) with 
     the same characteristics and loading behavior.
       </answer>
    </faq>

    <faq id="batch-file">
      <question>
     What is the batch configuration file?
      </question>
      <answer>
      Batch configuration file is a test plan for a batch of clients.
       </answer>
    </faq>

  </faqsection>
  <faqsection id="Hardware">
    <title>Hardware Requirements</title>
    <faq id="MinHW">
      <question>
     What are the minimal HW requirements?
      </question>
      <answer>
A PC with Pentium-III and 200 MB may be used for loads with hundreds simultaneous
loading clients.
       </answer>
    </faq>
   <faq id="RecHW">
      <question>
What is the recommended HW configuration?
      </question>
      <answer>
Note, that each virtual client takes about 30-35 K memory in user-space plus some none-pageable 
memory in kernel mainly for send and recv buffers. <br/>
<br/>
Our PC with Pentium-4 2.4 GHz and 480 MB memory is capable to support 3000-4000 simultaneously
loading clients. To reach 10K simultaneously loading clients we would recommend a PC with a
single 3.2 GHz Intel or 2.2 GHz AMD and 1.5-2 GB of memory.<br/>
<br/>
When loading from a multi-CPU (or multi-core) systems, a loading model with the number of
curl-loader processes equal to the number of CPUs may be most effective and deliver even higher
numbers (than 10K) of simultaneously loading clients from a single PC. If you are thinking, 
tuning, optimizing and getting our advise - the sky is the limit.
       </answer>
    </faq>
  </faqsection>

  <faqsection id="building">
    <title>Building curl-loader</title>
    <faq id="platform">
      <question>
Which operating systems are supported by curl-loader?
      </question>
      <answer>
You can use any operating system of your choice, providing that this is linux with kernel 2.4 or 2.6.
       </answer>
    </faq>
    <faq id="pre-requirements">
      <question>
What are the building pre-requirements?
      </question>
      <answer>
        General C development environment with bash, gcc, make, etc on a linux machine
        is required.<br/>
        <br/>
        Building pre-requirements are:<br/>
          1. openssl binaries;<br/>
          2. openssl development package with include files (on debian libssl-dev);<br/>
         <br/>
         Adjust Makefile variables to point to the openssl headers and libraries. 
         To specify an openssl development directory with include files (e.g. crypto.h), 
         export environment variable OPENSSLDIR with the value of that directory.<br/>
         For example:<br/>
         $export OPENSSLDIR=the-full-path-to-the-directory<br/>
         <br/>
         Another known issue is libidn.so, which means, that some linux distributions 
         do have some libidn.so.11, but not libidn.so. Resolve it by creating a softlink. <br/>
         <br/>
         Tarball of curl is included to the current release. When libcurl or curl-loader 
         have building issues, correct them in the Makefile. 
       </answer>
    </faq>
    <faq id="making">
      <question>
        How to make (build) curl-loader?
      </question>
      <answer>
          Run the following commands from your bash linux shell:<br/>
         $tar zxfv curl-loader-&lt;version&gt;.tar.gz<br/>
         $cd curl-loader-&lt;version&gt;<br/>
         $make<br/>
         <br/>
         By default, we are building both libcurl and curl-loader without optimization and with
         debugging -g option. To build with optimization and without debugging, please, run:<br/>
         $make cleanall<br/>
         $make optimize=1 debug=0<br/>
         If still any building issues, please, fill you free to contact us for assistance using placed in 
         download tarball PROBLEM-REPORTING form and its instructions.<br/>
       </answer>
    </faq>
    <!-- More faqs or parts here -->
  </faqsection>
  <faqsection id="configuration">
    <title>Creating Loading Configuration </title>
    <faq id="conf-file-general">
      <question>
How can I create loading configuration file?
      </question>
      <answer>
        <p>
To run a load create your configuration file to be passed to
curl-loader using -f commmand line option, e.g.<br/>
         <br/>
         #curl-loader -f ./conf-user/user_batch.conf<br/>
        <br/>
        For more examples, please, look at the files in "conf-examples" directory. You may 
        copy an example file and edit it by using the next FAQ guidelines.
        </p>
      </answer>
    </faq>
    
<faq id="conf-file-details">
      <question>
        What are the loading configuration file tags and semantics?
      </question>
      <answer>
       <p>
Configuration file or "batch configuration file" consists from tag=value
strings, groupped into 2 sections:<br/>
- General;<br/>
- URLs;<br/>
<br/>
Section General contains common for the batch parameters, whereas
section URLs contains one or more URL subsections each starting
with a certain URL and containing more tags.<br/>
<br/>
An example of a simple configuration file to be used for 
traffic generation client side against e.g. lighttpd web server:
----------------------------------------------------------------------------<br/>
########### GENERAL SECTION ##################<br/>
BATCH_NAME= 10K-clients<br/>  
CLIENTS_NUM_MAX=10000<br/>
CLIENTS_NUM_START=100<br/>
CLIENTS_RAMPUP_INC=50<br/>
INTERFACE   =eth0<br/>    
NETMASK=255.255.0.0 <br/> 
IP_ADDR_MIN= 192.168.1.1<br/>
IP_ADDR_MAX= 192.168.53.255<br/>
CYCLES_NUM= -1<br/>
URLS_NUM= 1<br/>
<br/>
########### URLs SECTION #######################<br/>
<br/>
URL=http://localhost/index.html<br/>
URL_SHORT_NAME="local-index"<br/>
REQUEST_TYPE=GET<br/>
TIMER_URL_COMPLETION = 0<br/>
TIMER_AFTER_URL_SLEEP = 0<br/>
<br/>
<br/>
The name of the batch is "10K-clients" with the maximum number of clients 
to be 10000; load to be started with initial 100 clients and each seconds will be added 50
clients more.<br/>
<br/>
For load will be used network interface eth0 with unique IP-address for each
client taken from 192.168.1.1 up to 192.168.53.255 and netmask 255.255.0.0.
Number of cycles is to be performed by each client in not limited (CYCLES_NUM= -1)
and will go forward till user presses Cntl-C to stop the load. Each client fetches
a single url (URLS_NUM= 1) http://localhost/index.html.<br/>
<br/>
The url is named as "local-index" and the name that will be used for Load Status GUI.
The url will be fetched using HTTP GET, and the operation will not be limited
in time (TIMER_URL_COMPLETION = 0). If we would like to limit this operation,
we could place here some number in milli-seconds, e.g. 3000 to be monitored and
enforced and reported, if missed, at Load Status GUI and other statistics. After
completing fetching url each client waits/sleeps for 0 ms 
(TIMER_AFTER_URL_SLEEP = 0, which means, that actually does not wait/sleep) 
and makes the next URL fetch.  <br/>
 <br/>
<br/>
Yet a one more example of a configuration file to be used login to a web-service,
performing some activity and logoff is below:<br/>
<br/>
########### GENERAL SECTION ###################<br/>
#<br/>
BATCH_NAME=web-service<br/>
CLIENTS_NUM_MAX=100<br/>
CLIENTS_RAMPUP_INC=2<br/>
INTERFACE=eth0 <br/>   
NETMASK=24  <br/>
IP_ADDR_MIN=192.168.0.2<br/>
IP_ADDR_MAX=192.168.0.2<br/>
URLS_NUM=4<br/>
CYCLES_NUM= 200<br/>
<br/>
########### URLs SECTION #######################<br/>
<br/>
### Login URL -  cycling <br/>
<br/>
# GET-part<br/>
# <br/>
URL=http://192.168.0.1:8888/vax/root/Admin<br/>
URL_SHORT_NAME="Login-GET"<br/>
REQUEST_TYPE=GET<br/>
TIMER_URL_COMPLETION = 10000<br/>
TIMER_AFTER_URL_SLEEP =5000<br/>
<br/>
# POST-part<br/>
# <br/>
URL=""<br/>
URL_SHORT_NAME="Login-POST"<br/>
URL_USE_CURRENT=1<br/>
USERNAME= admin<br/>
PASSWORD= admin-passphrase<br/>
REQUEST_TYPE=POST<br/>
FORM_USAGE_TYPE= SINGLE_USER<br/>
FORM_STRING= username=%s&amp;password=%s <br/>
TIMER_URL_COMPLETION = 4000  <br/>
TIMER_AFTER_URL_SLEEP =1000 <br/>
 <br/>
### Cycling URL <br/>
# <br/>
URL=http://192.168.0.1:8888/vax/Admin/ServiceList.do<br/>
URL_SHORT_NAME="Service List"<br/>
REQUEST_TYPE=GET<br/>
TIMER_URL_COMPLETION = 3000<br/>
TIMER_AFTER_URL_SLEEP =1000<br/>
<br/>
### Logoff URL - cycling, uses GET and cookies to logoff.<br/>
#<br/>
URL=http://192.168.0.1:8888/vax/Admin/Logout.do<br/>
URL_SHORT_NAME="Logoff"<br/>
REQUEST_TYPE=GET<br/>
TIMER_URL_COMPLETION = 0<br/>
TIMER_AFTER_URL_SLEEP =1000<br/>
<br/>
<br/>
The name of the batch is "web-service", maximum number of clients 100,
load to be started with initial 2 client and each seconds will be added 2
clients more.<br/>
<br/>
For load will be used network interface eth0 and the same IP-address for each
client 192.168.0.2 and netmask 255.255.255.0 (24 in CIDR notation). If we
would like to assign to each client a unique IP, it could be done by providing
a range of at least 100 IP-address, e.g. IP_ADDR_MIN=192.168.0.10 and 
IP_ADDR_MAX=192.168.0.110.
Number of cycles is to be performed by each client is limited to 200, and the load
stops, when each client does 200 cycles. Each client fetches up to 4 urls, specified 
in section  URLs<br/>
<br/>
The first URL operation, which is doing each simulated user/client is login. Login is
performed by GET-ing the page (with 3xx HTTP redirections handled inside)
with the first url http://192.168.0.1:8888/vax/root/Admin, called shortly "Login-GET". 
Maximum time, that we consider appropriate for this operation is 10000 msecs. 
The 2000 msec "sleep" timer simulates here the time spended by user to fill the 
username and password to the POST-form received by the GET.<br/>
<br/>
The second url is empty and is used for POST-ing to the "current" url, using the form.
Such empty string urls are allowed only, when URL_USE_CURRENT=1 is specified.
The url is named as "Login-POST". Username "admin" and password "admin-passphrase"
are used as the single user type and "filled" to the FORM_STRING= username=%s&amp;password=%s. 
After "filling", the resulting form, which is sent to the web-service is 
"username=admin&amp;password=admin-passphrase". There are other options available for FORM_STRING, 
such as unique generated users, unique generated passwords or credentials loaded from a file; 
these options will be described later. The maximum time allowed for the POST is 4000 msecs, 
and the time of waiting/sleeping till the next url is 1000 msecs. 
Worth to mention, that when the authentication is accomplished successfully,
server sets to client cookies and redirects the client to the service page, which is the next url
(note, that all redirections are handled internally).
 <br/>
 <br/>
The third url is http://192.168.0.1:8888/vax/Admin/ServiceList.do with a short name
"Service List". After the url is done a client comes to the forth url 
http://192.168.0.1:8888/vax/Admin/Logout.do to for a logoff. This batch (testing plan)
does not control and limit the time spended for logoff, which is indicated by
TIMER_URL_COMPLETION = 0. After doing a 1000 msecs sleeping, a client comes
back to the first url to repeat the sequence till the number of cycles is expired. <br/>
 <br/>
If we would like, that login and logoff to be done for each client only one, we would
add to the first, second and forth url the tag URL_DONT_CYCLE=1 to mark the urls
 as not-cycled to be fetched only once. <br/>
 <br/>

Note, that both quotted and non-quotted string are supported as the tags values.<br/>
<br/>
For more examples, please, look at the files in "conf-examples" directory.<br/>
        </p>
      </answer>
    </faq>

    <faq id="tag-description">
      <question>
        What are the exact meanings of all these tags?
      </question>
      <answer>
<br/>
Tags of the sections GENERAL:<br/>
<br/>
BATCH_NAME - a short string to be provided. The string will be
used to create logfile, statistics file and client dump files with the
same name as the batch-name and extensions .log, .txt and .ctx, respectively.<br/>
<br/>
CLIENTS_NUM_MAX - a maximum number of clients to be used for load.
Any positive number is valid here. Note, that each loading client requires 30-35 K
of RAM plus some non-pageable memory in kernel. We have already tried loads
with up to 20000 clients. Loads above 1000 client normally start with a some
lower starting number, managed by the next tag CLIENTS_NUM_START and
ramped-up each second by adding CLIENTS_RAMPUP_INC clients to the load.<br/>
<br/>
CLIENTS_NUM_START - number of clients to start the load with. If the tag is 
not present or zero, CLIENTS_RAMPUP_INC value will be taken instead. 
If neither CLIENTS_NUM_START nor CLIENTS_RAMPUP_INC are present
or have positive value, the loader will take CLIENTS_NUM_MAX as the number
of clients to start the load.<br/>
<br/>
CLIENTS_RAMPUP_INC - number of clients to be added to the load in the automatical
mode every second till CLIENTS_NUM_MAX number will be reached. For machines
with a single CPU we would recommend not to keep the number above 50-100, whereas
for dual-CPU machines you can keep it as large as 200-300.<br/>
<br/>
INTERFACE - name of the loading network interface. Find your interfaces by running
/sbin/ifconfig.<br/>
 <br/>
NETMASK - netmask for the loading clients IP-addresses (from IP_ADDR_MIN to 
IP_ADDR_MAX). For IPv4 you can use either quad-dotted netmask string or CIDR 
number from 0-32. For IPv6 only CIDR values from 0 to 128 are supported.<br/>
<br/>
IP_ADDR_MIN and IP_ADDR_MAX are the interval of IP-addresses to be used
for the clients. The interval of the addresses should be large enough to supply
a dedicated IP for each client. Another mode, that can be used is the same IP
for all clients; when IP_ADDR_MIN and IP_ADDR_MAX contain the same valid
address, it will be used for all clients. If the IP-addresses specified by the tags do not exist,
curl-loader adds them as the secondary IP-addresses to the loading network interface.<br/>
Remember, that you should ensure smooth routing from the addresses used to server
and from server back to the client addresses. Another route to ensure, when is appropriate,
is to/from DNS server for resolving.<br/>
<br/>
CYCLES_NUM is the number of cycles to be performed. Any positive value is
valid here as well as (-1), which means cycle indefinetely. Loading can be normally
stopped at any moment by pressing Cntl-C; all statistics will be collected properly and files
closed to contain necessary information. Note, that cycling relates only to the URLs not
containing tag URL_DONT_CYCLE = 1. Curl-loader enables two optional non-cycling
areas: the first before the cycling url/s area and the second after it.<br/>
<br/>
USER_AGENT provides an option to over-write the default MSIE-6-like HTTP header
User-Agent. Place here a quotted string to emulate the browser, that you need.
The header is entered globally. If you need an option to customize it on a per-URL bases,
please, use tag HEADER="User-Agent: the string of my favorite browser".<br/>
<br/>
URLS_NUM is the number of urls to be used in URLs Section.<br/>
<br/>
<br/>
Tags of the sections URLs:<br/>
<br/>
URL - is the first tag of each url-subsection and marks beginning of a new
url. The tag should be either a valid url or an empty string, which is allowed
only when URL_USE_CURRENT=1 presents. "Current url" is used for example, when 
POST-ing at the url, resulting from the previous GET. HTTP redirections
are handled internally and there is no need to use "current url" approach for that
purpose. Note, that URL used for file uploading should include a filename and
not only a directory.<br/>
<br/>
URL_SHORT_NAME - an optional up to 12 ASCII string long name, which will appear on a Load
Status GUI. It improves appearance of the current status at the console. Recommended
to configure it.<br/>
<br/>
URL_USE_CURRENT - used in combination with an empty string URL="" to indicate,
that the current url, fetched by the previous operation, should be used."Current url" is 
useful e.g., when POST-ing at the url, resulting from the previous GET. Note, that 
HTTP redirections are handled internally and there is no need to use "current url" 
approach for that purpose. <br/>
<br/>
URL_DONT_CYCLE - marks a url as non-cycling. Curl-loader enables two optional 
non-cycling areas: the first before the cycling url/s area and the second after it.
The first non-cycling area may be used for login, authentication, etc. single time
operations, whereas the second non-cycling area is due to a single time logoff, for example.<br/>
<br/>
REQUEST_TYPE - HTTP request method to be choosen from GET, POST or PUT.<br/>
<br/>
UPLOAD_FILE - filename, including path, to the file to be used for uploading.
The path should be taken from the curl-loader place, e.g. ./conf-examples/bax.conf <br/>
<br/>
HEADER - is assisting to customize/add/over-write HTTP/FTP headers. 
If a header already exits by default, the custom header over-writes it. 
USER_AGENT tag is for User-Agent header only, whereas by HEADER may be added 
or over-written any header (including User-Agent). Look for example at 
./conf-examples/custom_hdrs.conf.<br/>
<br/>
USERNAME and PASSWORD are the tags to provide credentials for the login operations.
The strings may be used either as is or as the base-words to append numbers and
to generate some unique credentials. The behavior is governed by the tag 
FORM_USAGE_TYPE. Note, that PASSWORD may be an empty string.<br/>
<br/>
FORM_USAGE_TYPE governs user login process. "UNIQUE_USERS_AND_PASSWORDS" 
is used to generate unique usernames and passwords by appending client sequence numbers 
(starting from 1) to USERNAME and PASSWORD tags within FORM_STRING template. 
"UNIQUE_USERS_SAME_PASSWORD" to be used when generating unique users with 
the same password. "SINGLE_USER" is useful, when all clients are making login using the 
same USERNAME and PASSWORD. "RECORDS_FROM_FILE" means, that user credentials 
to be loaded from the file specified by FORM_RECORDS_FILE value. "AS_IS" means, that
the FORM_STRING template to be used just AS IS and without any usernames or passwords 
either from the relevant tags or from file.<br/>
<br/>
<br/>
FORM_STRING is the configurable template form to be used for POST-ing credentials
ot any other records.<br/>
To generate multiple unique users with unique passwords FORM_USAGE_TYPE=
"UNIQUE_USERS_AND_PASSWORDS", and the FORM_STRING to be a template like
"username=%s%d&amp;password=%s%d". First '%s' will be substituted by the 
value of USERNAME tag and '%d' by the client number. Second '%s' will
be substituted by PASSWORD tag value and second '%d' by the same client
number. For example, if USERNAME=robert, PASSWORD=stam
and FORM_STRING "username=%s%d&amp;password=%s%d", the final POST string, 
used for the client number 1, will be  "username=robert1&amp;password=stam1".<br/>
In this case USERNAME and PASSWORD strings are used just 
as base-words for generating unique user credentials by appending an number.          
<br/>
When FORM_USAGE_TYPE="UNIQUE_USERS_SAME_PASSWORD" template
of FORM string to be something like "username=%s%d&amp;password=%s", where
only username will be unique using the same approach as described above.
<br/>
When FORM_USAGE_TYPE="SINGLE_USER", provide FORM_STRING without 
%d symbols, e.g. "user=%s&amp;secret=%s". Thus, all clients will have the same
POST credentials with the string looking like "user=robert&amp;secret=stam". 
<br/>
When FORM_USAGE_TYPE= "RECORDS_FROM_FILE" FORM_STRING to be also 
without %d symbols, because the credentials uniqueness is supposed to be ensured by 
the file content.<br/>
FORM_USAGE_TYPE="AS_IS" means, that FORM_STRING will not be validated and
POST-ed indeed AS IS.<br/>
<br/>
<br/>
FORM_RECORDS_FILE specifies the path to the file with credentials or any other tokens.
(full-path or relative to curl-loader). A text file with usernames and passwords 
with the structure of each string like: &lt;teken1&gt;&lt;separator&gt;&lt;token2&gt;
can be used as an input. According to RFC1738, only reserved ':', '@', '/' and probably 
' ' (space) are safe to use as separators between username and password. An example of 
batch configuration is ./conf-examples/post-form-token-fr-file.conf and an 
example of credentials is in ./conf-examples/credentials.cred.<br/>
We are planning to extend the file to include in each string up to 8 tokens (2 tokens now).<br/>
<br/>
WEB_AUTH_METHOD to be configured from the "BASIC", "DIGEST", "GSS_NEGOTIATE",
"NTLM" or "ANY". The configured method will be used for authentication on 
HTTP 401 response. When "ANY" is configured, libcurl will choose a method.
To use "GSS_NEGOTIATE" the libcurl should be re-compiled with support for GSS.<br/>
<br/>
WEB_AUTH_CREDENTIALS to be provided in the form "user:password".
If not configured, the loader uses USERNAME and PASSWORD tags value
to synthesize the credentials.<br/>
<br/>
PROXY_AUTH_METHOD to be configured from the "BASIC", "DIGEST", "GSS_NEGOTIATE",
"NTLM" or "ANY". The configured method will be used for authentication on 
HTTP 407 response. When "ANY" is configured, libcurl will choose a method.
To use "GSS_NEGOTIATE" the libcurl should be re-compiled with support for GSS.<br/>
<br/>
PROXY_AUTH_CREDENTIALS to be provided in the form "user:password".
If not configured, the loader uses USERNAME and PASSWORD tags value
to synthesize the credentials.<br/>
<br/>
FRESH_CONNECT is used to define on a per url bases, whether the connection 
should be re-used or closed and re-connected after request-response cycle. 
When 1, the TCP connection to be re-established. The system default is to keep 
the connection and re-use it as much as server and protocol allow it. Still the system
default could be changed by the command-line option -r.<br/>
<br/>
TIMER_TCP_CONN_SETUP  is the time in seconds for DNS resolving and TCP 
connection setup on a per url bases. The global default is 5 seconds, which can 
be changed using -c command-line option.<br/>
<br/>
TIMER_URL_COMPLETION is the time in milli-seconds to allow a 
url fetching operation, including all internal re-directions. The legal values are 0, 
which means no limit on url-completion time, or above 20 msecs. When a value of 
the tag is above 0, we are monitoring the url-fetching and cancelling it, 
if above the specified milliseconds. The results are presented on the Load Status 
GUI as the operation "Timeouted" statistics and logged to all statistics files as 
T-Err number.<br/>
<br/>
TIMER_AFTER_URL_SLEEP is the time in milli-secondsfor client to sleep 
after fetching a URL prior to the next URL to do. Zero (0) means don't wait 
and schedule client after the URL immediately.<br/>
<br/>
FTP_ACTIVE when defined as 1 forced FTP from the default passive to
the active mode.<br/>
<br/>
LOG_RESP_HEADERS when defined as 1 logs response headers to a file.
Directory &lt;batch-name&gt; is created with subdirs url0, url1, url&lt;n&gt;
Headers of responses are logged to the files named:
cl-&lt;client-num&gt;-cycle-&lt;cycle-num&gt;.hdr<br/>
<br/>
LOG_RESP_BODIES when defined as 1 logs response headers to a file.
Directory &lt;batch-name&gt; is created with subdirs url0, url1... url&lt;n&gt;
Headers of responses are logged to the files named:
cl-&lt;client-num&gt;-cycle-&lt;cycle-num&gt;.body<br/>
<br/>
      </answer>
    </faq>

    <faq id="login-logoff">
      <question>
        How does the loader support login, logoff and authentication flavors?
      </question>
      <answer>
       curl-loader performs login and logoff operations using the following HTTP methods:<br/>
         - GET+POST (server response to GET provides a post-form to be 
            filled and posted by POST);<br/>
          - POST only;<br/>
           - GET only.<br/>
          <br/>
         The loader supports HTTP Web Authentication and Proxy Authentication.
         The supported authentication methods are Basic, Digest (RFC2617) and NTLM.
         When responded 401 or 407, libcurl will choose the most safe method
         from those, supported by the server, unless the batch configuration (test plan)
         is explicitly indicating the method to be used by the tags WEB_AUTH_METHOD 
         and/or PROXY_AUTH_METHOD.<br/>
         To support GSS Web-Authentication, add in Makefile building of libcurl
         against appropriate GSS library, see libcurl pages for detailed instructions.<br/>
      </answer>
    </faq>
   </faqsection>

<faq id="ftp">
      <question>
     What about FTP load and mixed FTP/HTTP load?
      </question>
      <answer>
To generate FTP/FTPS load, please, pass user credentials via ftp-url according to 
the RFC 1738 like: ftp://username:password@hostname:port/etc-str<br/>
Tag FTP_ACTIVE serves to switch from the default passive FTP to active.
Please, look at the examples in conf-examples directory, e.g. ftp.conf, ftp_http.conf, 
ftp_upload.conf.<br/>
      </answer>
</faq>

  <faqsection id="Running">
    <title>Running Load</title>
    <faq id="environment">
      <question>
     What are the running environment requirements?
      </question>
      <answer>
      Running hundred and thousand of clients, please, do not forget:<br/>
        - to increase limit of descriptors (sockets) by running e.g. <br/>
           #ulimit -n 19999;<br/>
        - optionally, to set reuse of sockets in time-wait state: by setting <br/>
            #echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle and/or<br/> 
           #echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse;<br/>
       <br/>
In some cases you may need to increase the system limits for open
descriptors (sockets):<br/>
echo 65535 > /proc/sys/fs/file-max <br/>
Note, that your linux distribution may require editing of some other files
like /etc/security/limits.conf, etc
       </answer>
       </faq>

   <faq id="command-line">
      <question>
     How I can run the load?
      </question>
      <answer>
        Usage: run as a root user:<br/>
        #./curl-loader -f &lt;configuration filename&gt; [other options]<br/>
         <br/>
         Other possible options are:<br/>
        -c[onnection establishment timeout, seconds]<br/>
        -e[rror drop client. Client on error doesn't attempt loading any more]<br/>
        -h[elp]<br/>
        -i[ntermediate (snapshot) statistics time interval (default 3 sec)]<br/>
        -f[ilename of configuration to run (batches of clients)]<br/>
        -l[ogfile max size in MB (default 1024). On the size reached, file pointer is rewinded]<br/>
        -m[ode of loading, 0 - hyper (the default, epoll () based ), 1 - smooth (select () based)]<br/>
        -r[euse connections disabled. Closes TCP-connections and re-open them. Try with and without]<br/>
        -s[tderr printout of client messages instead of to logfile - attn!- bulky]<br/>
        -v[erbose output to the logfiles; includes info about headers sent/received]<br/>
        -u[rl logging - logs url names to logfile, when -v verbose option is used]<br/>
        -w[arnings skip]<br/>
        <br/>
       Connection Reuse Disable Option (-r):<br/>
       The default behavior of curl-loader after HTTP response is to re-use the 
       tcp-connection for the next request. If you are specifying -r command-line 
       option, the TCP connection will be closed and re-opened for the next request. 
       Whether it is appropriate for you to work with -r option or without, it  depends
       on your server support of Keep-Alive and the purpose of your testing.
       Try with and without -r and see, what you get.<br/>

      Connection reuse (which is the default, without -r option) has advantages due to the 
      decreased consumption of opened descriptors (sockets) and ports.<br/>
       </answer>
       </faq>

   <faq id="loading-modes">
      <question>
     Which loading modes are supported?
      </question>
      <answer>
Hyper-mode (command line -m0) is the default mode . The mode is using for event 
demultiplexing epoll() or /dev/epoll. <br/>
<br/>
Another loading mode is called "smoothing" (-m1 command line) is basically the same
as hyper, but uses poll() system call for demultiplexing.
<br/>
       </answer>
       </faq>

   <faq id="loading-status">
      <question>
     How I can monitor loading progress status?
      </question>
      <answer>
curl-loader outputs to the console loading status and statistics as the 
Load Status GUI ouput, where the left colomn is for the latest interval 
and the right is for the summary numbers since load start. 
A copy of the output is also saved in the file &lt;batch-name&gt;.txt<br/>
       </answer>
       </faq>

    <faq id="connect-timeout">
      <question>
     Why I am getting "Connection time-out after 5108 ms" like errors in the log file?
      </question>
      <answer>
This means, that the connections  cannot be established within  5 seconds 
(which is the default). If some of clients are getting such errors, this means, 
that the server is overloaded and cannot establish TCP connection within 
the time. Y may wish to increase the connection timeout globally by e.g. -c 10 or to
specify an increased timeout using tag TIMER_TCP_CONN_SETUP=10.<br/>
<br/>
When all your clients have such errors, in most cases this is due either to 
resolving or routing problems or other network problems.
Y can set any addresses to your clients in IP_ADDR_MIN and IP_ADDR_MAX tags, 
providing, that you ensure a smooth route from client and from server for the 
addresses you use.<br/>
<br/>
Please, start with a single client and see that it works, than progress further.
When you are specifying any addresses here, they are added by curl-loader
initialization to the network interface of your choice and may be seen by the command:<br/>
#/sbin/ip addr<br/>
<br/>
Command ping (man ping) has an option -I to force the ping to be issued from
a certain ip-address. To test, that your routing is OK, you may use, e.g.:<br/>
#ping -I &lt;the client address you wish to use&gt; url-taget<br/>
<br/>
When it works - it will be OK.
Y may also wish to read a bit into the linux routing/networking and DNS resolving
HOWTOS.<br/>
<br/>
       </answer>
    </faq>

  <faq id="logfile">
      <question>
     Where is the detaled log of all virtual clients activities and how to read it?
      </question>
      <answer>
Detailed log is written to the file named:<br/>
&lt;batch-name&gt;.log:<br/>
<br/>
The semantics of logfile output, using command line options -v (verbous) and -u 
(url print):<br/>
"Cycle number", "Client number (ip-address)" - some information string, e.g.:<br/>
<br/>
4 39 (192.168.0.39)  :== Info:   Trying 10.30.6.42... : eff-url: http://10.30.6.42:8888/server/Admin/ServiceList.do, url:<br/>
<br/>
Which meas: cycle: 4, client number 39 with ipv4 address (192.168.0.39), 
status of the message is Info, eff-url - is the url, used right now, "url:" is 
empty, which means, that it is the same as effective.<br/>
<br/>
Effective url may be a result of redirection and, thus, "url:" <br/>
(target url, specified in batch configuration file) will be printed as well.<br/>
<br/>
Please, note, that when the logfile reaches 1024 MB size, curl-loader
rewinds it and starts to overwrite it from the beginning. Y may tune
the rewinding file size by using command line option:<br/>
 -l &lt;log-filesize-in-MB&gt;<br/>
      </answer>
       </faq>
   <faq id="statistics">
      <question>
     Which statistics is collected and how to get to it?
      </question>
      <answer>
Currently HTTP/HTTPS statistics includes the following counters:<br/>
   - run-time in seconds;<br/>
   - requests num;<br/>
   - 1xx success num;<br/>
   - 2xx success num;<br/>
   - 3xx redirects num;<br/>
   - client 4xx errors num;<br/>
   - server 5xx errors num;<br/>
   - other errors num, like resolving, tcp-connect, server closing or 
     empty responses number;<br/>
   - url completion time expiration errors;
   - average application server Delay (msec), estimated as the time between 
     HTTP request and HTTP response without taking into the account network
     latency (RTT);<br/>
   - average application server Delay for 2xx (success) HTTP-responses, as above,
     but only for 2xx responses. The motivation for that is that 3xx redirections
     and 5xx server errors/rejects may not necessarily provide a true indication
     of a testing server working functionality.<br/>
   - throughput out (batch average);<br/>
   - throughput in (batch average);<br/>
<br/>
The statistics goes to the screen (both the interval and the current summary 
statistics for the load) as well as to the file with name &lt;batch_name&gt;.txt
When the load completes or when the user presses CTRL-C (sometimes some clients
may stall), the final load report is printed at the console as well as to the statistics
file.<br/>
<br/>
Some strings from the file:<br/>
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br/>
Run-Time,Appl,Clients,Req,1xx,2xx,3xx,4xx,5xx,Err,T-Err,D,D-2xx,T-In,T-Out<br/>
2, Appl , 100, 155, 0, 0, 96, 0, 0, 0, 0, 1154, 1154, 2108414, 15538<br/>
2, Sec-Appl, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0<br/>
4, Appl, 100, 75, 0, 32, 69, 0, 0, 0, 0, 1267, 1559, 1634656, 8181<br/>
4, Sec-Appl, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0<br/>
<br/>
Cutted here<br/>
<br/>
36, Appl , 39, 98, 0, 35, 58, 0, 0, 0, 0, 869, 851, 1339168, 11392<br/>
36, Sec-Appl, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0<br/>
38, Appl , 3, 91, 0, 44, 62, 0, 0, 0, 0, 530, 587, 1353899, 10136<br/>
38, Sec-Appl, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0<br/>
*, *, *, *, *, *, *, *, *, *, *, *<br/>
Run-Time,Appl,Clients,Req,1xx,2xx,3xx,4xx,5xx,Err,T-Err,D,D-2xx,T-In,T-Out<br/>
38, Appl , 0, 2050, 0, 643, 1407, 0, 213, 0, 0, 725, 812, 1610688, 11706<br/>
38, Sec-Appl, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0<br/>
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------<br/>
The bottom strings after asterisks are for final averages.<br/>
<br/>
At the same time a clients dump file with name &lt;batch_name&gt;.ctx is generated
to provide detailed statistics about each client state and statistics counters.<br/>

One string from the file:<br/>
1 (192.168.0.1) ,cycles:124,cstate:1,b-in:22722029,b-out:174605,req:745,2xx:497,3xx:248,4xx:0,5xx:0,err:0<br/>
where<br/>
1 (192.168.0.1)- is the index of the client and its ip-address;<br/>
cycles- number of loading cycles done;<br/>
cstate - is the number of the client state (-1 - error, 0 - init, 1- login, 2- uas, 3-logoff, 4-final-ok);<br/>
b-in - bytes passed in;<br/>
b-out - bytes passed out;<br/>
req- number of requests done;<br/>
2xx, 3xx, 4xx, 5xx - number of responses Nxx received;<br/>
err - number of libcurl errors at the resolving, TCP/IP and TLS/SSL levels;<br/>
<br/>
<br/>
The following conditions are considered as errors:<br/>
- error at the level of libcurl, which includes resolving, TCP/IP and, when applicable,
  TLS/SSL errors;<br/>
- all HTTP 5xx server errors;<br/>
- most of HTTP 4xx client errors, excluding 401 and 407 authentication responses
  not considered real errors;<br/>
When the above error conditions occur, a virtual client is marked as being 
in the error state.<br/>
<br/>
By default we "recover" such client by scheduling it to the next loading cycle, 
starting from the first operation of the cycle. You may use command line option
-e to change the default behavior to another, so that clients once arriving at 
error state will not be scheduled for load any more. <br/>
<br/>
       </answer>
       </faq>
  </faqsection>

  <faqsection id="Advanced">
    <title>Advanced Issues</title>

    <faq id="performance">
      <question>
     What about performance?
      </question>
      <answer>
HYPER mode (-m 0 command line option) was already used for loads with up to 10000 clients,
whereas HW issues, like CPU and memory resource, start to be counting.
The mode uses epoll() syscall (via libevent library) for demultiplexing. <br/>
<br/>
       </answer>
    </faq>

   <faq id="big-load">
      <question>
     How to run a really big load?
      </question>
      <answer>
0. Every big load starts with a small load. First, see, that you have a working 
configuration file. Run it with a 1-2-3 clients and commandline option -v, and
look into the l&lt;batch-name&gt;.log logfile.<br/>
Look into the HW issues, duscussed in the FAQs and mind, that
each client requires 30-35 K of memory.<br/>
<br/>
1. Compile with optimization;<br/>
Since you need performance compile with optimization and without debugging.<br/>
$make cleanall <br/>
$make optimize=1 debug=0<br/>
Y may add to Makefile optimization for your particular processor by 
-match /-mcpu gcc option directives to OPT_FLAGS.<br/>
<br/>
2. Login as a su;<br/>
<br/>
3. Increase the default number of allowed open descriptors (sockets);<br/>
<br/>
Run e.g. #ulimit -n 19900<br/>
<br/>
When running several instances of curl-loader, consider increase of system
limits for open descriptors, if necessary. Take your own account of the
socket usage in the system, considering sockets faster recycling (less
time in the time-wait state), by setting, optionally, something like this:<br/>
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle and/or  <br/>
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse;<br/>
<br/>
Correct, if required, the value of CURL_LOADER_FD_SETSIZE (set to 20000 in 
Makefile) to control the maximum fd, that may be used for select. This is not required
to be cared about for the default hyper mode.<br/>
<br/>
Increase the maximum number of open descriptors in your linux system, if required,
using linux HOWTOS.<br/>
echo 65535 > /proc/sys/fs/file-max <br/>
Note, that your linux distribution may require editing of some other files
like /etc/security/limits.conf, etc<br/>
<br/>
4. Relax routing checks;<br/>
<br/>
Relax routing checks for your loading network interface. When "eth0" used for loading  run:<br/>
echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter<br/>
echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter<br/>
<br/>
5. Increase memory available for kernel tcp;<br/>
Read the maximum available TCP memory and sysctl or echo as a root
the number to the kernel tcp, e.g.:<br/>
cat /proc/sys/net/core/wmem_max - the output is 109568.<br/>
/sbin/sysctl net.ipv4.tcp_mem="109568 109568 109568" or <br/>
echo "109568 109568 109568" > /proc/sys/net/ipv4/tcp_mem<br/>
<br/>
6. Create configuration files for each instance of curl-loader to run.<br/>
<br/>
What is important is to give a unique value for tag BATCH_NAME,
which is in use by a separate instance of curl-loader. Logfile, report file, etc 
have name, which are the derivatives of the BATCH_NAME value. 
Therefore, when several instances of curl-loader are writing to the same file, 
this is not helpful and may be even "crashful". Please, use in your configuration 
batch files non-overlapping ranges of IP-addresses, else libcurl virtual clients 
will compete for the IP-addresses to bind to.<br/>
<br/>
Use CLIENTS_RAMPUP_INC tag in smooth or hyper mode to increase number of your 
clients gradually at start-up in order not boom the server and not to burn out the CPU
at your loading machine. Addition of new clients is a CPU-expensive operation, therefore
keep CLIENTS_RAMPUP_INC below 100 clients per second.<br/>
<br/>
7. Connections re-use.<br/>
<br/>
The default behavior of curl-loader now is to re-use the tcp-connection for the next 
request. This default decreases consumption of CPU and open sockets.
If you are specifying -r command-line option, the connection will be closed and 
re-opened for the next request. <br/>
<br/>
8. Troubleshooting.<br/>
<br/>
Run the first loading attempt with a small number of clients using command-line 
options -v (verbose) and -u (url in logs). Grep to look for the errors and their reasons. 
If an error is "Connection timeout", you may try to increase the connection establishment
timeout (the default is 5 seconds), using -c command-line option. When all your clients
fail to connect with "Connection timeout" error, this may be due to routing or resolving 
problems.<br/>
If any assistance required, please, don't hesitate to contact us using 
PROBLEM-REPORTING form in the download tarball.<br/>
<br/>
9. Logs and statistics.<br/>
<br/>
After end of a run, or after SIGINT (Cntl-C), the final results are calculated
and printed to the console as well as to the file &lt;batch-name&gt;.txt.
Current results are presented in each row, and average summary as the last
raws, separated from the rest by asterisks.<br/>
<br/>
Pay attention, that &lt;batch-name&gt;.log log file may become huge, particularly, 
when using verbose output (-v -u). Command-line option -l &lt;maxsize in MB&gt; may 
be useful, whereas the default policy is to rewind the logfile (writing from the 
file start), when it reaches 1 GB. Do not use -v and -u options, when you have 
performance issues.<br/>
<br/>
10. Monitoring of the loading PC.<br/>
<br/>
Please, use top or vmstat commands to monitor the memory usage, swapping and CPU.
Intensive swapping is a good indication, that your PC is short in user-space memory.
If you see "memory pressure" counters in netstat -s output, this is a good indication,
that the PC is short in kernel memory for TCP.<br/>
Zero idle CPU is not the show stopper, but when you see, that Load Status GUI on 
your console prints its output with delays higher than it should be 
(the default is 3 seconds and may be adjusted by -i command-line option), you need
stronger CPU or run several curl-loader processes on a multi-CPU machine. Note, that 
for a load with several curl-loader precesses you need to arrange different
configuration files with different batch-names and not overlapping ranges of IP-addresses 
for each curl-loader process.<br/>
       </answer>
    </faq>

   <faq id="caps">
      <question>
How to calculate CAPS numbers for a load?
      </question>
      <answer>
When number of clients is defined by CLIENTS_NUM_MAX tag, number of
CAPS (call attempts per seconds) is resulting from the clients number and 
load duration for each cycle, comprising from:<br/>
- login time with possible redirections and sleeping after login interval;<br/>
- uas time for each url with possible redirections, intervals betwee urls and 
  after uas interval;<br/>
- logoff time with possible redirections and sleeping after logoff interval;<br/>
<br/>
The actions and time intervals are configurable in batch file, whereas
url retrival time is server and network dependent and not always easy 
to predict. The result is that number of clients/requests is a known parameter,
and number of CAPS is something to be estimated from the time of test and
number of requests.<br/>
<br/>
Smooth and hyper modes are presenting at the LOAD STATUS GUI the output 
of calculated current and average CAPS.<br/>
       </answer>
    </faq>
</faqsection>
  <!-- More faqs or parts here -->
</faqs>
