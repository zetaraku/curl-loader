HOWTOS-BIG-LOAD

Please, start from README.

These are the temporary instructions till we hopefully migrate
to epoll-based I/O demultiplexing (or even kevent based?),
supporting tens of thousand of clients in one thread without select/poll
limitations

1. Compile with optimization;
Since you need performance compile with optimization and without debugging.
$make cleanall 
$make optimize=1 debug=0
Y may add to Makefile optimization for your particular processor by 
-match /-mcpu gcc option directives to OPT_FLAGS.

2. Login as a su;

3. Increase the default number of allowed open descriptors (sockets);
 
E.g. running #ulimit -n 20000
If running several instances of curl-loader, consider increase of system
limits for open descriptors, if necessary. Take your own account of the
socket usage in the system, considering sockets faster recycling (less
time in the time-wait state), by setting, optionally, something like this:
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle and/or  
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse;

Correct, if required, the value of CURL_LOADER_FD_SETSIZE (set to 20000 in 
Makefile) to control the maximum fd, that may be used for select.

Increase the maximum number of open descriptors in your linux system, if required,
using linux HOWTOS.

4. Create configuration files for each instance of curl-loader to run.

What is important is to give a unique BATCH_NAME for each batch-file,
which is in use by a separate instance of curl-loader. Logfile, report file, etc 
have name, which are derivatives of the BATCH_NAME, and when several
instances of curl-loader are writing to the same file, this is not
helpful and crashful. Please, use in the configuration batch files 
non-overlapping ranges of IP-addresses, else libcurl virtual clients will 
compete for the IP-addresses to bind to.

Use CLIENTS_INITIAL_INC tag in smooth mode to increase number of your 
clients gradually at start-up in order not boom the server.


5. Connections re-use.

The default behavior of curl-loader now is after HTTP response to re-use 
the tcp-connection for the next request-response. 
If you are specifying -r command-line option, the connection will be closed and 
re-opened for the next request. Whether it is appropriate for you to work with
-r or without depends on your server support of Keep-Alive and the purpose 
of your testing.
Try with and without -r and see, what you get.

6. Try sometimes the non-default storming mode (commandline option -m1), 
which may better suit your needs. See, README for explanations. A combination of
-m1 and -r is a valid combination, still, if the server does not make problems 
with connections reuse.

7. Troubleshooting

Running first runs one may try command-line options -v (verbose) and -u (url
in logs). Grep to look for the errors and their reasons. If an error is
"Connection timeout", you may try to increase the connection establishment
timeout (the default is 5 seconds - huge, but "ih veis"), using -c command-line
option.

If any assistance required, please, don't hesitate to contact us.


8. Logs and statistics.

After end of a run, or after SIGINT (Cntl-C), the final results are calculated
and printed to the console as well as to the file <$BATCH_NAME>.txt.
Current results are presented in each row, and average summary as the last
raws, separated from the rest by asterisks.

Pay attention, that <$BATCH_NAME>.log log file may become huge, particularly, 
when using verbose output (-v -u). Commandline option -l <maxsize in MB> may 
be useful, whereas the default policy is to rewind the logfile (writing from the 
file start), when it reaches 1 GB. Otherwise, do not use -v and -u options for 
rather long loads.
