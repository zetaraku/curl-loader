Last updated: July 2 2007

Nearest release 0.33 may hopefully include: 

1. FETCH_PROBABILITY tag with a value from 0 to 100
     percents will provide an option to fetch a url not by all
    but rather by a fraction of clients. When the tag is 30 (%),
    it means, that only 30% of clients will fetch the url, whereas
    70% of clients will go forward.
    FETCH_PROBABILITY_ONCE=1 will enable to use random
    calculation only once (at the first cycle), where a client will remember
    the decision whether to fetch a url or not, and will use the result of the
    once made decision for all cycles.

2. Usage of random time intervals, e.g 100-1000 means a random time from 
    100 to 1000 msec.

3. HTML analyses. libcurl example is using libtidy in:
   http://www.koders.com/c/fid7923BF828D0534D6B747ED138FCDD50DCFD13B3F.aspx

4. A regular expression pattern for a search in a response body
   Look in Apache JMeter.

5. Regular expression operations for the header/s certain header and whether 
   value matches a regular expression pattern.


---------------------------------------------------------------------------------------------------- 

Next releases:

0. Performance improvements:

   -cached memory allocators, using libcurl API 
   (Aleksandar Lazic <al-curlloader@none.at>):
   http://curl.haxx.se/libcurl/c/curl_global_init_mem.html
   Michael Moser has proposed to use the memory allocator like here:
   http://www.hoard.org/
   http://www.cs.umass.edu/%7Eemery/hoard/asplos2000.pdf

- thread affinity for curl-loader SMP/multi- core HW adaptation feature.
- Testbed for 50K and 100K clients. We need a more powerful HW:
    2-4 CPUs/cores and 4-8 GB of memory.

1. Multipart post forms (RFC1867) can be enhanced by adding loading tokens
    from file, generation of unique words, etc 

2. FETCH_REPETITION tag to define the number to repeat a url prior to going
    to the next one.

3. Logging to files headers and bodies of responses exists. We may wish to improve
   it on a certain stage by using memory-mapped files from our custom write
   finctions. 
   Memory-mapped files are effective for logging, but always we know the size in
   advance. Mapping -re-mapping and truncation, if taken too much, may be 
   required.

4. Configuration/making improvements: moving all source-files
   to src directory etc.

5. X.509 client certificates per each https, ftps url. Client certificate
   for each client?

6. Support for CAPS (calls per second) defined mode. Currently, we are 
   supporting only a certain number of virtual clients and CAPS is the derived
   parameter. CAPS-MODE MAY support a certain number of CAPS, 
   and virtual clients number to be the derived parameter;

7. Load Status GUI. Decrease of loading clients number [-|\] - SIPP-like;

8. Template-guided output of configurable statistics 
   (what user wishes with desired string, names) to statistics file;

9. Support for more protocols: telnet, SFTP, SCP, SSH, etc;

10. Support for more HTTP and FTP features.

11. Support for browsers bevavior simulation.

   Aleksandar Lazic <al-curlloader@none.at> has suggested to mimic
   the behavior of browsers/wget, etc by:

   "set max_connections_per_site <NUM>
   set deep_or_breadth          (DEEP|BREADTH)
   set deep_or_breadth_count    <NUM>
   set wait_human_read_time     <NUM>

   repeat as long as deep_or_breadth_count is not reached {

   get site (e.g.: index.html)
   parse site and ( count links and get the needed elements from remote
                server
             || if element is to get make new connection but not more
                the max_connections_per_site
              )

   wait_human_read_time()

   if breadth && deep_or_breadth_count not reached {
      get the next link from same directory-level (e.g.: /, /news/, ...)
    } elsif deep && deep_or_breadth_count not reached {
       get the next link from the next directory-level
    }
   }"

   Daniel Stenberg <daniel@haxx.se> has mentioned:
   "The http spec recommends using no more than two connections to a single
   server, and I believe they intened the primary connection to be used to get
   the main html, and then you do a second connection (with pipelining) to get
   all the images etc that the main HTML refers to.

   For libcurl, you would add an easy handle for each connection you want to
   make. libcurl's pipelining support doesn't currently allow you to do any fine-
   grained control on what gets pipelined or not, so if you enable it you'll get
   pipelining for every subsequent request sent to the same server."


12. Vladislav Wainbaum proposed to add extraction of a field from response
      to be used in the next requests.
