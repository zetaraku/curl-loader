Last updated: 21/06/2007

Nearest release 0.33 may hopefully include: 


1. Performance improvements:

   -cached memory allocators, using libcurl API 
   (Aleksandar Lazic <al-curlloader@none.at>):
   http://curl.haxx.se/libcurl/c/curl_global_init_mem.html
   Michael Moser has proposed to use the memory allocator like here:
   http://www.hoard.org/
   http://www.cs.umass.edu/%7Eemery/hoard/asplos2000.pdf

- thread affinity for curl-loader SMP/multi- core HW adaptation feature.

2. HTML analyses. libcurl example is using libtidy in:
   http://www.koders.com/c/fid7923BF828D0534D6B747ED138FCDD50DCFD13B3F.aspx

3. A regular expression pattern for a search in a response body
   Look in Apache JMeter.

4. Regular expression operations for the header/s certain header and whether 
   value matches a regular expression pattern


---------------------------------------------------------------------------------------------------- 

Next releases:

0. Performance improvements:
- Testbed for 50K and 100K clients. We need a more powerful HW:
    2-4 CPUs/cores and 4-8 GB of memory.

1. Multipart post forms (RFC1867) can be enhanced by adding loading tokens
    from file, generation of unique words, etc 

2. An option to download a url not only once a cycle, but according to its
   "Weight", probability. Weight can be less than 1, e.g. 0.3, which means, that 
    there is a 30 % probability, that a client will load this url. Weight 3 means,
    that there is 300 % probability, that a url will be fetched, which practically
    means, that a url will be fetched several times by clients prior to going to next
    and with an average of fetching it 3-times. 

4. Logging to files headers and bodies of responses exists. We may wish to improve
   it on a certain stage by using memory-mapped files from our custom write
   finctions. 
   Memory-mapped files are effective for logging, but always we know the size in
   advance. Mapping -re-mapping and truncation, if taken too much, may be 
   required.

5. Aleksandar Lazic <al-curlloader@none.at> has suggested to make configurable 
   a number of TCP connections, that our virtual client can use (the default 
   is up to 5), and, if persistant, what is the number of request-responses 
   till the connection to be closed and re-established. 
   Another proposal of Aleks is to make configurable HTTP 1.1 or 1.0, that
   we will not implement unless it becomes a feature required by many users.
   We can add to libcurl a new option - something like 
   CURLOPT_CONNECTION_REUSENUM to pass a <max_number> of requests till 
   connection refresh.
   Note, that httperf (sessions) is a reference application.

6. Configuration/making improvements: moving all source-files
   to src directory etc.

7. X.509 client certificates per each https, ftps url. Client certificate
   for each client?

8. Support for CAPS (calls per second) defined mode. Currently, we are 
   supporting only a certain number of virtual clients and CAPS is the derived
   parameter. CAPS-MODE MAY support a certain number of CAPS, 
   and virtual clients number to be the derived parameter;

9. Load Status GUI. Decrease of loading clients number [-|\] - SIPP-like;

10. Template-guided output of configurable statistics 
   (what user wishes with desired string, names) to statistics file;

11. Usage of random time intervals, e.g 100-200 (from 100 to 200 msec);

12. Support for more protocols: telnet, SFTP, SCP, SSH, etc;

13. Support for more HTTP and FTP features.

14. Support for browsers bevavior simulation.

   Aleksandar Lazic <al-curlloader@none.at> has suggested to mimic
   the behavior of browsers/wget, etc by:

   "set max_connections_per_site <NUM>
   set deep_or_breadth          (DEEP|BREADTH)
   set deep_or_breadth_count    <NUM>
   set wait_human_read_time     <NUM>

   repeat as long as deep_or_breadth_count is not reached {

   get site (e.g.: index.html)
   parse site and ( count links and get the needed elements from remote
                server
             || if element is to get make new connection but not more
                the max_connections_per_site
              )

   wait_human_read_time()

   if breadth && deep_or_breadth_count not reached {
      get the next link from same directory-level (e.g.: /, /news/, ...)
    } elsif deep && deep_or_breadth_count not reached {
       get the next link from the next directory-level
    }
   }"

   Daniel Stenberg <daniel@haxx.se> has mentioned:
   "The http spec recommends using no more than two connections to a single
   server, and I believe they intened the primary connection to be used to get
   the main html, and then you do a second connection (with pipelining) to get
   all the images etc that the main HTML refers to.

   For libcurl, you would add an easy handle for each connection you want to
   make. libcurl's pipelining support doesn't currently allow you to do any fine-
   grained control on what gets pipelined or not, so if you enable it you'll get
   pipelining for every subsequent request sent to the same server."
