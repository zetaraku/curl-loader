    curl-loader, version 0.18, 12/10/2006

    Robert Iakobashvili, Ashdod, Israel, coroberti@gmail.com
    Michael Moser, Modiin, Israel, moser.michael@gmail.com

ABOUT:
    Provides application load, simulating thousands (running from
a script several curl_loaders - tens of thousands) clients, each with
its own source IP-address.
    This version emulates HTTP/HTTPS clients. All activities of each client are
logged to a file: connection establishment, request headers sent, response headers
received, data received and all TCP as well as HTTP and TLS (for HTTPS) errors. 
The tool can be easily extended to support ftp, ftps, telnet, tftp, ldap, etc other 
application protocols, supported by the great libcurl library.
    Clients are groupped to the so-called batches of clients, performing the same
sort of activities. Configurable activities of the simulated clients are:
- authentication login;
- user activity simulation by fetching several urls and timeouts in between;
- logoff.


SUCCESSFULLY USED:

1. To generate Gbps traffic from thousands of TCP clients to test 
    thousands of firewalling and NAT iptables/ipset entries impact 
    at CPU consumption of a gateway device. 
    Curl_loader provided client load against Apache web-server fetching a 
    url with a gigabyte file, thus creating a permanent heavy-load 
    traffic contaning thousand of tcp-streams at the gateway in the middle.

2. To emulate http and https clients redirection, authentication and simulate
    user-behaviour and real load for thousands of such clients to test 
    authentication gateway. 
    Curl_loader provided client load against Apache web-server with the gateway 
    in the middle, where the gateway performed browser hijecking and HTTP- 
    redirection of the curl-clients to the HTTPS url at the gateway own 
    web-server. HTTPS page of the gateway web-server provided a POST form,
    filled with username and password for its further authentication against
    external AAA (RADIUS) server. If the authentication was OK, user (curl
    client) was allowed to enter the Internet and download several urls, etc 
    perform some sort of emulated by curl_loader network activity.   
    

MAY BE USED:
1. To create testing load for various HTTP/HTTPS/FTP/FTPS/LDAP, etc devices, 
    where libcurl supports various variants and flavors of the protocols;
2. For testing of HTTP/HTTPS based applications, application servers;
2. For etc, etc, etc applications.

OS SUPPORT:
Currently linux due to actually unlimited number of secondary ip-addresses,
that can be arranged at a network interface to bind each individual client
to an IP-address.
Supposed to work on linux 2.4 and 2.6 kernels.

BUILDING:
In most cases just run make and relax. For some linux distributions, however, 
matters should be fine tuned in build_curl.sh and Makefile. Sorry.
Tarball of curl should be fetched by wget from the curl web-site. 
If a computer does not have Internet connection, bring the package yourself 
and put it inside to the distribution directory.
We are normally building it at Debian. Sometimes, libcurl has issues with
buidling at RedHat/FC, where it requires linking with libidn. Correct it
in the Makefile. 
To build with optimization, please, follow the instructions in Makefile and
build_curl.sh

CONFIGURATION:
curl_loader -h is your friend. Some examples of configuration are suggested
in the files located in "configs" directory.

ENVIRONMENT: ATTENTION !!! IMPORTANT !!!
Running hundlers and thousands of clients, please do not forget:
    - to increase limit of desriptors (sockets) by running e.g. 
    #ulimit -n 10000;
    - optionally, to set reuse of sockets in time-wait state: by setting 
    #echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle and/or  
    #echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse;

PERFORMANCE ISSUES:
A limit of 1000 clients per batch (due to select) may be worked around by 
a script, which runs several instances of curl_loader, each with 1000 clients.
Running several batches with threads (option -t), each with 1000 clients is 
another valid option, however, may be less stable - try it.
Some future release will hopefully incorporate libcurl HYPER API, using 
curl_multi_socket() approach and epoll support by libevent as in
http://curl.haxx.se/lxr/source/hiper/hipev.c


MODES:
The default mode is "storming", where all clients of a batch are starting 
together. They are expected to finish their job within a certain timeout.
After the timeout they are either completed fetching url, or are cutted
and disconnected and a new cycle begins.
Another mode is a so-called "smooth" (-m2 command line), where each 
client spends as much time as wishes and starts another url or another 
cycle only after completing the previous url.
A combination of the both modes (two separate instances of the program)
can provide somehow realistic behavior with some smooth constant load
and bursts of requests.

LICENSE:
Actually it is GPL2 due to the code from iprouted2. However, if somebody
fills uncomfortable with this license, we can substitute the code of
ip_secondary.c to calls to command "ip", and release it under BSD-license,
and/or make it configurable.

TERMINOLOGY:
--- What is the batch?
Batch is a group of clients with the same characteristics and 
loading behavior. Configuration file has one or more batches.
--- What means UAS?
UAS - user activity simulation - fetching url, sleeping, another url -sleeping
cycles, simulating actual user activity.

COMMAND-LINE AND CONFIG FILE
Usage: run as a root user:
#./curl-loader -f <configuration filename> [other options]

Possible options are:
-c[onnection establishment timeout, seconds]
-f[ilename of configuration to run (batches of clients)]
-l[ogfile rewind after this number of cycles]
-o[utput to stdout bodies of downloaded files - attn!- bulky]
-r[euse established connections, don't run close-open cycles]
-s[tderr printout of client messages instread of to logfile - attn!- bulky]
-t[hreads enable - enables threads, each runs a batch of clients]
-v[erbose output to the logfiles; includes info about headers sent/received]
-u[rl logging - enables logging of url names to all client outputs]

Note, that running with threads (-t) may be less stable,
whereas without this option runs only the first batch of clients specified.
Thus, running several client batches without threads requires some script with
several curl-loader processes.

Configuration file should possess at least one client batch defined
with the following params in each batch:

----------------------------------------------------------------------------
########### GENERAL SECTION ################################
BATCH_NAME= bulk_batch     # The name of the batch. Logfile - bulk_batch.log
CLIENTS_NUM=3              # Number of clients in the batch
INTERFACE = eth0           # Name of the network interface from which to load 
NETMASK=20                 # Netmask in CIDR-notation (number of bits with 1)
IP_ADDR_MIN= 192.168.1.1   # The client addresses starting address
IP_ADDR_MAX= 192.168.5.255 # Redundant, for self-control
CYCLES_NUM= 100            # Number of loading cycles to run, 0 -forever

########### LOGIN SECTION ##################################
LOGIN=n          # If 'y' or 'Y', login enabled, all other tags of the 
                  # section to be filled. If 'n' or 'N' - comment out others
#LOGIN_USERNAME=  # Usename string
#LOGIN_PASSWORD=  # Password string
#LOGIN_REQ_TYPE=  # Use a number to be either GET+POST (1), or POST-only (2).
#LOGIN_POST_STR=  # POST string matrix. See below:
#
# To generate multiple unique users with unique passwords, use the string like
# "username=%s%d&password=%s%d". First %s will be substituted by the 
# value of LOGIN_USERNAME tag and %d by the client number. Second %s will
# be substituted by LOGIN_PASSWORD tag value and second %d by the same client
# number. For example, if LOGIN_USERNAME=robert, LOGIN_PASSWORD=stam
# and LOGIN_POST_STR "username=%s%d&password=%s%d" will be formed the
# string for POST for the client number 1 as "username=robert1&password=stam1".
# In such case LOGIN_USERNAME and LOGIN_PASSWORD strings are used just 
# as bases for generating unique user credentials.
#
# To use the username and password 'as as', just provide LOGIN_POST_STR without 
# %d symbols, e.g. "user=%s&secret=%s". Thus, all clients will have the same
# POST credentials with the string looking like "user=robert&secret=stam".
#
# Note, that the words like 'username', 'user', 'password', 'secret', etc are 
# just those fields, that login users are required to fill in their POST page.

#LOGIN_URL=                 # A valid http or https url to be used for login
#LOGIN_URL_MAX_TIME=        # Maximum batch time in seconds to login
#LOGIN_URL_INTERLEAVE_TIME= # Time in seconds to sleep after login
#LOGIN_CYCLING=             # If 'y' login should be run in cycles, and not 
                            # just done only once

########### UAS SECTION ####################################
UAS=y            # If 'y' or 'Y', login enabled, and other lines of the section 
                 # to be filled
UAS_URLS_NUM = 2 # Number of urls will be taken as in the format below

UAS_URL=http://localhost/apache2-default/ACE-INSTALL.html
UAS_URL_MAX_TIME = 6        # Maximum batch time in seconds to fetch the url
UAS_URL_INTERLEAVE_TIME = 0 # Time in seconds to sleep after fetching the url

UAS_URL= http://localhost/apache2-default/index.html
UAS_URL_MAX_TIME = 4        # Maximum batch time in seconds to fetch the url
UAS_URL_INTERLEAVE_TIME = 0 # Time in seconds to sleep after fetching the url

########### LOGOFF SECTION #################################
LOGOFF=n                 # If 'y' or 'Y', login enabled, and other tags of the 
                         # section to be filled. If 'n' or 'N' - comment out others
#LOGOFF_REQ_TYPE=        # Use numbers for GET-only (1), GET+POST (2), POST-only (3)
#LOGOFF_POST_STR=        # String to be used for logoff, like "op=logoff" 
#LOGOFF_URL=             # A valid http or https url to be used for logoff
#LOGOFF_URL_MAX_TIME=    # Maximum batch time in seconds to logoff
#LOGOFF_URL_INTERLEAVE_TIME= # Time in seconds to sleep after logoff
#LOGOFF_CYCLING=         # If 'y' login should be run in cycles, and not just 
                         # done only once
--------------------------------------------------------------------------------

Worth to mention, that each batch configuration should contain all tags from
the section GENERAL as well as LOGIN, UAS and LOGOFF section tags. When 
LOGIN, UAS or LOGOFF is set as 'y' (yes), all tags for that section to appear 
(uncommented) and to be set to some valid values (N/A string to be used not 
to define *POST_STR tags, when POST is not applicable). In the case, that LOGIN, 
UAS or LOGOFF section tag is disabled by setting 'n' (no) value, thus, the tags 
of the disabled section should not appear or just to be commented out by '#'.

For more examples, please, look at configs directory.

Note, that currenly there is a limit of 1000 sockets per batch of clients.
Running thousands and more clients, please do not forget the options:
- to increase limit of open desriptors in shell by running e.g. "#ulimit -n 10000":
- to increase total limit of  open descriptors in systeme somewhere in /proc
- to consider reusing sockets in time-wait state: by "#echo 1 > 
  /proc/sys/net/ipv4/tcp_tw_recycle"
- and/or  "#echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse"

There are two login modes supported. GET+POST (1) and POST-only (2). 
When both username and password are provided, login can be made using the
first url only once for each client either for GET with following POST or
for POST only (note, that all 3xx HTTP redirections are supported as
build-ins by libcurl).

There are three logoff modes supported as GET-only, GET+POST and POST-only,
which stay for 1, 2 and 3 numbers. 

Cookies are enabled in curl-loader by default. 
