    curl-loader, version 0.21

    Robert Iakobashvili, Ashdod, Israel, coroberti@gmail.com
    Michael Moser, Modiin, Israel, moser.michael@gmail.com

ABOUT:
    Provides application load using libcurl/openssl stacks, simulating behaviour
of thousands and tens of thousands clients, each with its own source IP-address.
    This version simulates HTTP/HTTPS and FTP/FTPS (experimental) clients. 
Activities of each virtual client are logged to file: resolving, connection
 establishment, sending of requests, receiving responses, headers and data 
received/sent, errors from network, SSL and application (HTTP, FTP) levels. 
The tool can be easily extended to generate telnet, tftp, ldap, etc other 
application load, supported by the great libcurl library.
    Virtual clients are grouped to the so-called batches of clients, performing
the same sort of activities. Configurable activities for the simulated clients are:
- authentication login;
- user activity simulation by fetching several urls and timeouts in between;
- logoff.


SUCCESSFULLY USED:

1. To simulate HTTP/S load of thousands of clients against an authentication 
   gateway for testing of the gateway performance in various scenarios.

    curl-loader supplied HTTP/S client load against Apache web-server with the 
    gateway in the middle, where the gateway made a browser hijacking and HTTP- 
    redirection of the curl-clients to the HTTPS url at the gateway's own 
    web-server. HTTPS page of the web-server provided a POST form to the user
    with username and password for the client/user authentication against
    an external AAA (RADIUS) server. If the authentication was OK, user (a libcurl
    virtual client object) was allowed to enter the Internet and to perform some
    sort of simulated by curl-loader network activity, namely, fetching urls and 
    sleeping in between them.   
    
2. To test web-server pages, authenticating tens and hundred thousand of
    clients, where each client comes to a HTTPS url using GET method and is
    redirected by the web-server to another url, providing authentication POST 
    form with username and password. After successful authentication of a client
    the web-server sets to the client cookies. Client activities are further 
    simulated by fetching urls and sleeping in between. Clients are doing logoff 
    using GET-method to the web-server logoff-url, where the set cookies are used
    by the web-server as the client identity.

3. To generate Gbps traffic from thousands of TCP/HTTP clients and to test the 
    impact of thousands of firewalling and NAT iptables/ipset rules and 
    hundreds of the rules being added/deleted each second at performance of a 
    gateway device.

    curl-loader provided client load against Apache web-server fetching a 
    url with a gigabyte file, thus, creating a permanent heavy-load 
    traffic, containing thousand of tcp-streams at the gateway in the middle.

MAY BE USED:
1. To create testing load for various HTTP/HTTPS/FTP/FTPS/LDAP, etc devices, 
    where libcurl supports various variants and flavors of the protocols;
2. For testing of HTTP/HTTPS, FTP/FTPS based applications, application servers;
3. For etc, etc, etc applications.

OS SUPPORT:
Supposed to work on linux 2.4 and 2.6 kernels.

BUILDING:
In most cases just run make and relax. For some linux distributions, however, 
matters should be fine tuned in build_curl.sh and Makefile (libidn issues, etc). 

Building requirements include openssl libraries as well as development package
with include files. Adjust the build_curl.sh and Makefile variables to point
to the openssl headers and libraries. If you want to specify an openssl 
directory, export environment variable OPENSSLDIR with the value of
that directory.

Another known issue is linidn.so, which means, that some linux distributions 
do have some libidn.so.11, but not libidn.so. Resolve it by creating a softlink.

Tarball of curl is included to the current release. When libcurl of curl-loader 
have building issues, correct them in the Makefile and build_curl.sh. To build 
with optimization, please, follow the instructions in Makefile and build_curl.sh.

If still any building issues, please, fill you free to contact us for assistance.


CONFIGURATION:
./curl_loader -h output is your friend. Some examples of configuration are 
suggested in the files, located in "configs" directory.


ENVIRONMENT: ATTENTION !!! IMPORTANT !!!
Running hundreds and thousands of clients, please do not forget:
    - to increase limit of descriptors (sockets) by running e.g. 
      #ulimit -n 10000;
    - optionally, to set reuse of sockets in time-wait state: by setting 
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle and/or  
      #echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse;

PERFORMANCE ISSUES:
There is no more a limit of 1000 clients per batch due to select FD_SETSIZE,
so you can try 2000 and may be more number of clients from the single thread
(select). Please, care about available sockets and limits. System call select () 
does not scale well with a big number of sockets, therefore, you can try a 
script, which runs several instances of curl_loader, each with 1000-2000 
clients.

Running several batches with threads (option -t), each with 1000-2000 clients is 
another valid option, however, may be less stable - try it.

Some future release will hopefully incorporate libcurl HYPER API, using 
curl_multi_socket() approach and epoll API support by libevent as in
http://curl.haxx.se/lxr/source/hiper/hipev.c
Read also HOWTOS-BIG-LOAD.


MODES:
One of the modes is storming mode (-m1 command line), where all clients 
of a batch are starting together. They are expected to finish their job within 
a certain timeout. After the timeout they are either completed fetching url, 
or are cutted and disconnected and a new cycle begins. 

Another mode is a so-called "smooth" (the default mode or -m2 command line), 
where each client spends as much time as wishes and starts another url or 
another cycle only after completing the previous url. When a client loading is
terminated due to some reason, e.g. connection timeout, the default behavior
is to schedule loading for this client at the next cycle. If -e option is passed to
command line, the client will not be re-scheduled any more, which is usefull
to get indication of errors by monitoring drop in number of active clients. Look
in logfile for errors, and when connection timeout error appears, adjust the
connection timeout using -c command line option. Note, that the smooth mode
is suitable for login-logoff cycles of load. Still, the limitation in TODO is, that
it loads url after url without respecting the interleave timeouts. It will be
hopefully corrected in next versions.

When the first operation is login, storming mode may require large 10-20-30 
sec time for all clients to accomplish. The indication of that may be 
"empty reply" ERROR from server in log file. Thus, smooth mode, where 
all clients are proceding independently, may be worth to consider. Storming
mode is better to use, when login operation is a single one and not in cycles, 
or when the tool is used just as a traffic generator. From another side
errors in storming mode are the good indications of the server not being 
able to coop with the burst of login requests, whereas smothing mode has a
somehow adaptive character to the server responses rate.

The problem of the both loading modes (particularly storming, where for smooth
it may be sometimes adjusted by -c option ) is that all clients are starting
loading immediately. It should be an option to configure calls-per-second
number (CAPS), and enable a gradual loading, lets say 200 clients start
loading each second, etc. The issue is also in out TODO list.

A combination of the both modes (two separate instances of the program)
can provide somehow realistic behavior with some "smooth" load plus
"storming" bursts of requests.


SMOOTH MODE LOADING OPTIONS:

First option is to define number of clients in CLIENTS_NUM. Thus, number of
CAPS (call per seconds) is derived from the clients number and duration of
load for each cycle comprising from:
- login time and sleeping after login interval;
- uas time for each url with possible redirections, intervals betwee urls and 
  after uas interval;
- logoff time and sleeping after logoff interval;
All these actions and time intervals are configurable in batch file, whereas
url retrival time is also server and network dependent and not always easy 
to predict. The result is that number of clients/requests is a known parameter,
and number of CAPS is something to be estimated from the time of test and
number of requests.
Tag CAPS_INITIAL_NUM in batch configuration file serves to assist for the first 
mode not schedule all clients simulteneous for load, but rather to smooth the 
initial requests flow for the first url.

Second option is to define number of CAPS explicitly by the tag 
CAPS_MODE_STEADY_NUM in the batch configuration file. Thus, every second 
will be scheduled or re-scheduled not more than CAPS_MODE_STEADY_NUM clients. 
Number of concurrent clients is defined by the CAPS number and the duration of 
a loading cycle as defined by the batch file configuration. Limit of concurrent 
clients number still needs to be predicted and defined in batch file 
CLIENTS_NUM tag as well as it should be allocated an appropriate range of 
ip-addresses for the clients.

To summarize:
Option 1: To define number of clients in CLIENTS_NUM and CAPS_INITIAL_NUM and
to get estimations of actual caps;
Option 2: To define the CAPS_MODE_STEADY_NUM and CAPS_INITIAL_NUM, if different
from the steady-state caps number, whereas number of clients will be reported
for the loader. Still, CLIENTS_NUM should be defined as the maximum limit of
the clients allowed in order to control the ranges of ip-addresses.


LICENSE:
Actually it is GPL2 due to the code from iprouted2. However, if somebody
fills uncomfortable with this license, we can substitute the code of
ip_secondary.c to calls to command "ip", and release it under BSD-license,
and/or make it configurable.

TERMINOLOGY:
--- What is the batch?
Batch is a group of clients with the same characteristics and 
loading behavior. Configuration file has one or more batches.
--- What means UAS?
UAS - user activity simulation - fetching url, sleeping, another url -sleeping
cycles, simulating actual user activity.


COMMAND-LINE OPTIONS
Usage: run as a root user:
#./curl-loader -f <configuration filename> [other options]

Possible options are:
-c[onnection establishment timeout, seconds]
-e[rror drop client (smooth mode). Client on error doesn't attempt loading any mode.]
-f[ilename of configuration to run (batches of clients)]
-l[ogfile rewind after this number of cycles]
-m[ode of loading, 1 - storming, 2 - smooth (default)]
-o[utput to stdout bodies of downloaded files - attn!- bulky]
-r[euse connections disabled. Close connections and re-open them. Try with and without]
-s[tderr printout of client messages instead of to logfile - attn!- bulky]
-t[hreads enable - enables threads, each runs a batch of clients]
-v[erbose output to the logfiles; includes info about headers sent/received]
-u[rl logging - logs url names to logfile, when -v verbose option is used]

Note, that curl-loader with threads (-t) may be less stable,
whereas without this option runs only the first batch of clients specified.
Thus, running several client batches without threads requires some script with
several curl-loader processes.

Connection Reuse Disable Option (-r):
The default behavior of curl-loader after HTTP response is to re-use the 
tcp-connection for the next request. If you are specifying -r command-line 
option, the TCP connection will be closed and re-opened for the next request. 
Whether it is appropriate for you to work with -r option or without, it  depends
 on your server support of Keep-Alive and the purpose of your testing.
Try with and without -r and see, what you get.

Connection reuse (without -r option) has advantages by decreasing
consumption of opened descriptors (sockets) and ports.


CONFIGURATION FILE
Configuration file should possess at least one client batch defined
with the following params in each batch:

----------------------------------------------------------------------------
########### GENERAL SECTION ################################
BATCH_NAME= bulk_batch     # The name of the batch. Logfile - bulk_batch.log
CLIENTS_NUM=3              # Number of clients in the batch
INTERFACE = eth0           # Name of the network interface from which to load 
NETMASK=20                 # Netmask in CIDR-notation (number of bits with 1)
IP_ADDR_MIN= 192.168.1.1   # The client addresses starting address
IP_ADDR_MAX= 192.168.5.255 # Redundant, for self-control
CYCLES_NUM= 100            # Number of loading cycles to run, 0 -forever

########### LOGIN SECTION ##################################
LOGIN=n           # If 'y' or 'Y', login enabled, all other tags of the 
                  # section to be filled. If 'n' or 'N' - comment out others
#LOGIN_USERNAME=  # Username string
#LOGIN_PASSWORD=  # Password string
#LOGIN_REQ_TYPE=  # Use string either GET+POST or POST
#LOGIN_POST_STR=  # POST string matrix. See below:
#
# To generate multiple unique users with unique passwords, use the string like
# "username=%s%d&password=%s%d". First '%s' will be substituted by the 
# value of LOGIN_USERNAME tag and '%d' by the client number. Second '%s' will
# be substituted by LOGIN_PASSWORD tag value and second '%d' by the same client
# number. For example, if LOGIN_USERNAME=robert, LOGIN_PASSWORD=stam
# and LOGIN_POST_STR "username=%s%d&password=%s%d", the final POST string, 
# used for the client number 1, will be  "username=robert1&password=stam1".
# In this case LOGIN_USERNAME and LOGIN_PASSWORD strings are used just 
# as base-words for generating unique user credentials by appending an number.
#
# To use the username and password 'as as', just provide LOGIN_POST_STR without 
# %d symbols, e.g. "user=%s&secret=%s". Thus, all clients will have the same
# POST credentials with the string looking like "user=robert&secret=stam".
#
# Note, that the words like 'username', 'user', 'password', 'secret', etc are 
# those fields, that login users are required to fill in their POST page.

#LOGIN_URL=                 # A valid http or https url to be used for login
#LOGIN_URL_MAX_TIME=        # Maximum batch time in seconds to login
#LOGIN_URL_INTERLEAVE_TIME= # Time in seconds to sleep after login
#LOGIN_CYCLING=             # If 'y' login should be run in cycles, and not 
                            # just done only once

########### UAS SECTION ####################################
UAS=y            # If 'y' or 'Y', login enabled, and other lines of the section 
                 # to be filled
UAS_URLS_NUM = 2 # Number of urls

UAS_URL=ftp://anonymous:stam@localhost/curl-7.16.0.tar.gz # Rather large file
UAS_URL_MAX_TIME = 20        # Maximum batch time in seconds to fetch the url
UAS_URL_INTERLEAVE_TIME = 0  # Time in seconds to sleep after fetching the url

UAS_URL= http://localhost/apache2-default/index.html
UAS_URL_MAX_TIME = 4        # Maximum batch time in seconds to fetch the url
UAS_URL_INTERLEAVE_TIME = 0 # Time in seconds to sleep after fetching the url

# You may add any number of urls providing 3-tags for each url as above,
# but do not forget to update the UAS_URLS_NUM.

########### LOGOFF SECTION #################################
LOGOFF=n                 # If 'y' or 'Y', login enabled, and other tags of the 
                         # section to be filled. If 'n' or 'N' - comment out others
#LOGOFF_REQ_TYPE=        # Use strings to be GET, GET+POST, or POST
#LOGOFF_POST_STR=        # String to be used for logoff, like "op=logoff" 
#LOGOFF_URL=             # A valid http or https url to be used for logoff
#LOGOFF_URL_MAX_TIME=    # Maximum batch time in seconds to logoff
#LOGOFF_URL_INTERLEAVE_TIME= # Time in seconds to sleep after logoff
#LOGOFF_CYCLING=         # If 'y' login should be run in cycles, and not just 
                         # done only once
--------------------------------------------------------------------------------

Worth to mention, that each batch configuration should contain all tags from
the section GENERAL as well as LOGIN, UAS and LOGOFF section tags. When 
LOGIN, UAS or LOGOFF is set as 'y' (yes), all tags for that section should 
appear (uncommented) and to be set to some valid values (N/A string to be used 
not to define LOGIN_POST_STR and LOGOFF_POST_STR tags, when POST 
is not applicable). In the case, that LOGIN, UAS or LOGOFF section tag is 
disabled by setting 'n' (no) value, thus, all the tags of the disabled section 
should not appear or just to be simply commented out by '#'.

For more examples, please, look at configs directory.

LOGIN/LOGOFF HTTP METHODS:
There are two login modes supported. GET+POST and POST-only. 
There are three logoff modes supported as GET-only, GET+POST and POST-only.
HTTP cookies are enabled in curl-loader by default.


FTP/FTPS LOAD (experimental):
To generate FTP/FTPS load, please, use UAS section and pass user credentials
via ftp-url according to the RFC 1738), like:
ftp://username:password@hostname:port/etc-str


LOGFILE:
The semantics of logfile output, using command line options -v (verbous) and -u 
(url print):
"Cycle number", "Client number (ip-address)" - some information string, e.g.:

4 39 (192.168.0.39)  :== Info:   Trying 10.30.6.42... : eff-url: http://10.30.6.42:8888/server/Admin/ServiceList.do, url:

Which meas: cycle: 4, client number 39 with ipv4 address (192.168.0.39), 
status of the message is Info, eff-url - is the url, used right now, "url:" is 
empty, which means, that it is the same as effective.

Effective url may be a result of redirection and, thus, "url:" 
(target url, specified in batch configuration file) will be printed as well.


STATISTICS (recently added feature):
Currently HTTP/HTTPS statistics includes the following counters:
   - requests num;
   - responses success num;
   - 3xx redirects num;
   - client 4xx errors num;
   - server 5xx errors num;
   - other errors num, like resolving, tcp-connect, server closing or 
     empty responses number;
   - average application server delay (msec), estimated as the time between 
     HTTP request and HTTP response without taking into the account network
     latency (RTT);
   - average application server delay for 2xx (success) HTTP-responses, as above,
     but only for 2xx responses. The motivation for that is that 3xx redirections
     and 5xx server errors/rejects may not necessarily provide a true indication
     of a testing server working functionality.
   - throughput out (batch average);
   - throughput in (batch average);

The statistics goes to the screen as well as to the file with name $batch_name.txt
When the load completes or when the user presses CTRL-C (sometimes some clients
may stall), the final load report is printed at the console as well as to the statistics
file. 
Some strings from the file:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Time, Protocol, Clients, Req, Redirs, Rsp-OK, Rsp-Serv-Err, Err, Delay, Delay-2xx, Thr-In, Thr-Out
2, HTTP, 100, 155, 0, 96, 0, 0, 1154, 1154, 2108414, 15538
2, HTTPS, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0
4, HTTP, 100, 75, 32, 69, 0, 0, 1267, 1559, 1634656, 8181
4, HTTPS, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0

Cutted here

36, HTTP, 39, 98, 35, 58, 0, 0, 869, 851, 1339168, 11392
36, HTTPS, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0
38, HTTP, 3, 91, 44, 62, 0, 0, 530, 587, 1353899, 10136
38, HTTPS, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0
*, *, *, *, *, *, *, *, *, *, *, *
38, HTTP, 0, 2050, 643, 1407, 0, 213, 725, 812, 1610688, 11706
38, HTTPS, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The bottom strings after asterisks are for final averages.

At the same time a clients dump file with name $batch_name.ctx is generated
to provide detailed statistics about each client state and statistics counters.

One string from the file
18 (192.168.1.18) ,cycles:5,state:4,b_in:549508,b_out:3940,req:17,rsp_3xx:5,rsp_oks:12,rsp_4xx:0,rsp_5xx:0,err:3

HOWTOS-BIG-LOAD
The file contains some useful tips.

LIMITATIONS:
See the TODO file for some of them.
